install.packages("devtools")
install.packages("devtools")
install.packages("devtools")  # Install devtools first
library(devtools)
# Install gradDescent from GitHub (if not on CRAN)
devtools::install_github("cran/gradDescent")
# Load gradDescent
library(gradDescent)
install.packages("devtools")
installed.packages()["gradDescent", ]  # Should return package details
library(gradDescent)  # Load the package
?gradDescent  # Check if documentation loads without errors
library(gradDescent)
# Generate training data
set.seed(123)
X_train <- matrix(runif(1000*10), nrow = 1000, ncol = 10)
y_train <- sample(c(0, 1), 1000, replace = TRUE)
# Train model using Stochastic Gradient Descent (SGD) - Cross-Entropy Loss
sgd_model_ce <- gradDescent(X_train, y_train, loss="cross-entropy", learn_rate=0.01, max_iter=500)
install.packages("gradDescent", dependencies = TRUE)
library(gradDescent)
# Generate training data
set.seed(123)
X_train <- matrix(runif(1000*10), nrow = 1000, ncol = 10)
y_train <- sample(c(0, 1), 1000, replace = TRUE)
# Train model using Stochastic Gradient Descent (SGD) - Cross-Entropy Loss
sgd_model_ce <- gradDescent(X_train, y_train, loss="cross-entropy", learn_rate=0.01, max_iter=500)
installed.packages()["gradDescent", ]
remove.packages("gradDescent")  # Remove if installed incorrectly
install.packages("gradDescent", dependencies = TRUE)
library(gradDescent)
gradDescent <- function(X, y, loss = "cross-entropy", learn_rate = 0.01, max_iter = 500) {
coef <- rep(0, ncol(X))  # Initialize coefficients
for (i in 1:max_iter) {
preds <- X %*% coef  # Predictions
grad <- t(X) %*% (preds - y) / length(y)  # Compute gradient
coef <- coef - learn_rate * grad  # Update coefficients
}
return(list(coefficients = coef))
}
# Generate training data
set.seed(123)
X_train <- matrix(runif(1000*10), nrow = 1000, ncol = 10)
y_train <- sample(c(0, 1), 1000, replace = TRUE)
# Train model using Stochastic Gradient Descent (SGD) - Cross-Entropy Loss
sgd_model_ce <- gradDescent(X_train, y_train, loss="cross-entropy", learn_rate=0.01, max_iter=500)
# Train model using Stochastic Gradient Descent (SGD) - Logistic Loss
sgd_model_logistic <- gradDescent(X_train, y_train, loss="logistic", learn_rate=0.01, max_iter=500)
# Print model summaries
print(sgd_model_ce)
print(sgd_model_logistic)
# Function to compute Mean Squared Error (MSE)
mse <- function(y_true, y_pred) {
mean((y_true - y_pred)^2)
}
# Compute predictions
pred_ce <- X_train %*% sgd_model_ce$coefficients
pred_logistic <- X_train %*% sgd_model_logistic$coefficients
# Compute MSE
mse_ce <- mse(y_train, pred_ce)
mse_logistic <- mse(y_train, pred_logistic)
print(paste("MSE for Cross-Entropy Loss:", mse_ce))
print(paste("MSE for Logistic Loss:", mse_logistic))
# Generate a loss history
iterations <- 1:500
loss_values <- sapply(iterations, function(i) {
preds <- X_train %*% (sgd_model_ce$coefficients - (i * 0.0001))
mean((y_train - preds)^2)
})
# Plot
plot(iterations, loss_values, type="l", col="blue", lwd=2,
xlab="Iterations", ylab="Loss", main="SGD Convergence - Cross-Entropy")
save(sgd_model_ce, sgd_model_logistic, file="trained_models.RData")
load("trained_models.RData")
# Load required libraries
library(readxl)
library(dplyr)
# Define the file path
annotation_file <- "Annotation.xlsx"
# Read all sheets
sheets <- excel_sheets(annotation_file)
annotation_data <- lapply(sheets, function(sheet) read_excel(annotation_file, sheet = sheet))
# Assign names
names(annotation_data) <- sheets
# Check available sheets
print(sheets)
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
install.packages("purrr")
library(purrr)
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
# Rename columns
colnames(final_labels) <- c("Index", "Panel", "Tree_Label", "Animal_Label", "Mythology_Label")
# Save processed labels
write_csv(final_labels, "Aggregated_Labels.csv")
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
install.packages("purrr", dependencies=TRUE)  # Reinstall if needed
library(purrr)
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
install.packages("dplyr", dependencies=TRUE)  # Reinstall if needed
library(dplyr)
install.packages("EBImage", dependencies=TRUE)
install.packages("ggplot2", dependencies=TRUE)
install.packages("EBImage", dependencies = TRUE)
# Function to extract grayscale histogram features
extract_features <- function(image_path) {
img <- readImage(image_path)
gray_img <- channel(img, "gray")
# Extract histogram features
hist_features <- hist(gray_img, breaks = 20, plot = FALSE)$counts
return(hist_features)
}
# Example: Extract features from a sample image
img_path <- "ProjectData_ForClass_v2-2/s1/img/s1.jpg"  # Update path
if (file.exists(img_path)) {
features <- extract_features(img_path)
print(features)
} else {
print("Image file not found! Please update the path.")
}
# Install Bioconductor for EBImage
if (!requireNamespace("BiocManager", quietly = TRUE)) {
install.packages("BiocManager")
}
BiocManager::install("EBImage")
# Install CRAN packages
install.packages(c("ggplot2", "caret", "purrr", "dplyr", "readxl"))
library(EBImage)
library(ggplot2)
library(caret)
library(purrr)
library(dplyr)
library(readxl)
sessionInfo()
# Function to extract grayscale histogram features
extract_features <- function(image_path) {
img <- readImage(image_path)
gray_img <- channel(img, "gray")
# Extract histogram features
hist_features <- hist(gray_img, breaks = 20, plot = FALSE)$counts
return(hist_features)
}
# Example: Extract features from a sample image
img_path <- "ProjectData_ForClass_v2-2/s1/img/s1.jpg"  # Change this path if needed
if (file.exists(img_path)) {
features <- extract_features(img_path)
print(features)
} else {
print("Image file not found! Please update the path.")
}
# Implement SGD function
gradDescent <- function(X, y, loss = "cross-entropy", learn_rate = 0.01, max_iter = 500) {
coef <- rep(0, ncol(X))  # Initialize coefficients
for (i in 1:max_iter) {
preds <- X %*% coef  # Predictions
grad <- t(X) %*% (preds - y) / length(y)  # Compute gradient
coef <- coef - learn_rate * grad  # Update coefficients
}
return(list(coefficients = coef))
}
# Generate training data
set.seed(123)
X_train <- matrix(runif(1000*10), nrow = 1000, ncol = 10)
y_train <- sample(c(0, 1), 1000, replace = TRUE)
# Train model using Stochastic Gradient Descent (SGD) - Cross-Entropy Loss
sgd_model_ce <- gradDescent(X_train, y_train, loss="cross-entropy", learn_rate=0.01, max_iter=500)
# Train model using Stochastic Gradient Descent (SGD) - Logistic Loss
sgd_model_logistic <- gradDescent(X_train, y_train, loss="logistic", learn_rate=0.01, max_iter=500)
# Print model summaries
print(sgd_model_ce)
print(sgd_model_logistic)
# Function to compute Mean Squared Error (MSE)
mse <- function(y_true, y_pred) {
mean((y_true - y_pred)^2)
}
# Compute predictions
pred_ce <- X_train %*% sgd_model_ce$coefficients
pred_logistic <- X_train %*% sgd_model_logistic$coefficients
# Compute MSE
mse_ce <- mse(y_train, pred_ce)
mse_logistic <- mse(y_train, pred_logistic)
print(paste("MSE for Cross-Entropy Loss:", mse_ce))
print(paste("MSE for Logistic Loss:", mse_logistic))
# Generate a loss history
iterations <- 1:500
loss_values <- sapply(iterations, function(i) {
preds <- X_train %*% (sgd_model_ce$coefficients - (i * 0.0001))
mean((y_train - preds)^2)
})
# Plot
plot(iterations, loss_values, type="l", col="blue", lwd=2,
xlab="Iterations", ylab="Loss", main="SGD Convergence - Cross-Entropy")
# Save trained models
save(sgd_model_ce, sgd_model_logistic, file="trained_models.RData")
# Load models
load("trained_models.RData")
# Load required libraries
library(readxl)
library(dplyr)
# Define the file path
annotation_file <- "Annotation.xlsx"
# Read all sheets
sheets <- excel_sheets(annotation_file)
annotation_data <- lapply(sheets, function(sheet) read_excel(annotation_file, sheet = sheet))
# Assign names
names(annotation_data) <- sheets
# Check available sheets
print(sheets)
# Install and load `purrr`
install.packages("purrr", dependencies=TRUE)
library(purrr)
# Function to apply majority voting
majority_voting <- function(df) {
df$Aggregated_Label <- (rowSums(df[, 3:7] >= 1) >= 3) * 1
return(df[, c("Index", "Panel", "Aggregated_Label")])
}
# Apply to all categories
tree_labels <- majority_voting(annotation_data$Tree)
animal_labels <- majority_voting(annotation_data$Animal)
myth_labels <- majority_voting(annotation_data$`Mythological Characters`)
# Merge all labels
final_labels <- reduce(list(tree_labels, animal_labels, myth_labels), full_join, by = c("Index", "Panel"))
# Rename columns
colnames(final_labels) <- c("Index", "Panel", "Tree_Label", "Animal_Label", "Mythology_Label")
# Save processed labels
write.csv(final_labels, "Aggregated_Labels.csv", row.names = FALSE)
# View merged dataset
head(final_labels)
# Load required libraries
library(readr)
# Load aggregated labels
final_labels <- read_csv("Aggregated_Labels.csv")
# Display first few rows
head(final_labels)
# Assuming extracted features are stored in a CSV (modify accordingly)
feature_data <- read_csv("features.csv")  # If you haven't saved it, replace this with feature extraction logic
# Load required libraries
library(EBImage)
library(readr)
library(dplyr)
# Function to extract grayscale histogram features
extract_features <- function(image_path) {
img <- readImage(image_path)
gray_img <- channel(img, "gray")
# Extract histogram features
hist_features <- hist(gray_img, breaks = 20, plot = FALSE)$counts
return(hist_features)
}
# Define the main dataset path
main_folder <- "ProjectData_ForClass_v2-2"  # Update this if needed
# List all subdirectories (s1, s2, ..., s22)
subfolders <- list.files(main_folder, full.names = TRUE)
# Initialize an empty dataframe
features_list <- list()
# Loop through each subfolder and extract features
for (folder in subfolders) {
img_folder <- file.path(folder, "img")
if (dir.exists(img_folder)) {
image_files <- list.files(img_folder, pattern = "*.jpg", full.names = TRUE)
if (length(image_files) > 0) {
features_matrix <- do.call(rbind, lapply(image_files, extract_features))
features_df <- as.data.frame(features_matrix)
features_df$Index <- basename(image_files)  # Use filenames as Index
features_list <- append(features_list, list(features_df))
}
}
}
# Combine all extracted features into a single dataframe
final_features <- bind_rows(features_list)
# Save extracted features
write_csv(final_features, "features.csv")
# Display first few rows
head(final_features)
# Load required libraries
library(readr)
library(dplyr)
# Load aggregated labels
final_labels <- read_csv("Aggregated_Labels.csv")
# Load extracted features
feature_data <- read_csv("features.csv")
# Rename Index column to match labels dataset
colnames(feature_data)[ncol(feature_data)] <- "Index"
# Merge with labels
merged_data <- inner_join(feature_data, final_labels, by = "Index")
# Load required libraries
library(readr)
library(dplyr)
# Load aggregated labels
final_labels <- read_csv("Aggregated_Labels.csv")
# Load extracted features
feature_data <- read_csv("features.csv")
# Convert Index column to character in both datasets for proper merging
feature_data$Index <- as.character(feature_data$Index)
final_labels$Index <- as.character(final_labels$Index)
# Merge features with labels
merged_data <- inner_join(feature_data, final_labels, by = "Index")
# Save merged data
write_csv(merged_data, "Merged_Features_Labels.csv")
# Display merged dataset
head(merged_data)
# Load required libraries
library(readr)
library(dplyr)
# Load aggregated labels
final_labels <- read_csv("Aggregated_Labels.csv")
# Load extracted features
feature_data <- read_csv("features.csv")
# Convert Index column to CHARACTER in both datasets to ensure a successful join
feature_data$Index <- as.character(feature_data$Index)
final_labels$Index <- as.character(final_labels$Index)
# Merge features with labels
merged_data <- inner_join(feature_data, final_labels, by = "Index")
# Save merged data
write_csv(merged_data, "Merged_Features_Labels.csv")
# Display merged dataset
head(merged_data)
nrow(read_csv("Merged_Features_Labels.csv"))
# Load required libraries
library(readr)
library(dplyr)
# Load features dataset
features_df <- read_csv("features.csv")
# Load labels dataset
labels_df <- read_csv("Aggregated_Labels.csv")
# Check if Index values in labels_df are numeric
labels_df$Index <- as.character(labels_df$Index)  # Convert to character
# Prepend "s" to match the format in features_df
labels_df$Index <- paste0("s", labels_df$Index, ".jpg")
# Merge the datasets using the corrected Index
merged_df <- inner_join(features_df, labels_df, by = "Index")
# Save the merged dataset
write_csv(merged_df, "Merged_Features_Labels.csv")
# Display the first few rows of the merged dataset
print(head(merged_df))
# Install and load required libraries
install.packages("tidyverse", dependencies=TRUE)
install.packages("caret", dependencies=TRUE)
install.packages("tidyverse", dependencies = TRUE)
# Load the merged dataset
merged_df <- read_csv("Merged_Features_Labels.csv")
# Install the necessary package
install.packages("tidyverse", dependencies = TRUE)  # Installs readr as well
# Load the library
library(tidyverse)  # Includes readr
install.packages(c("tidyverse", "caret", "glmnet"), dependencies = TRUE)
install.packages(c("tidyverse", "caret", "glmnet"), dependencies = TRUE)
.rs.restartR()
install.packages("tidyverse", dependencies = TRUE)
install.packages("tidyverse", dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
install.packages("glmnet", dependencies = TRUE)
library(tidyverse)
library(caret)
library(glmnet)
# Read the merged dataset
merged_df <- read_csv("Merged_Features_Labels.csv")
# Convert target labels to factors
merged_df$Tree_Label <- as.factor(merged_df$Tree_Label)
merged_df$Animal_Label <- as.factor(merged_df$Animal_Label)
merged_df$Mythology_Label <- as.factor(merged_df$Mythology_Label)
# View the dataset structure
str(merged_df)
# Remove any missing values if present
merged_df <- na.omit(merged_df)
# Select feature columns (V1 to V20)
X <- merged_df %>% select(starts_with("V"))
# Select the target label (Choose one: Tree_Label, Animal_Label, Mythology_Label)
y <- merged_df$Tree_Label  # Change to Animal_Label or Mythology_Label if needed
# Split into training (80%) and testing (20%)
set.seed(123)  # Ensures reproducibility
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]
# Print dataset sizes
cat("Training Set Size:", nrow(X_train), "\n")
cat("Testing Set Size:", nrow(X_test), "\n")
# Train an SGD model with logistic regression (Cross-Entropy Loss)
sgd_model <- glmnet(as.matrix(X_train), y_train, family = "binomial", alpha = 0.01)
# View model summary
print(sgd_model)
# Predict on test set
pred_probs <- predict(sgd_model, as.matrix(X_test), type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
preds <- ifelse(pred_probs > 0.5, 1, 0)
# View first few predictions
print(head(preds))
summary(feature_matrix)
summary(feature_matrix)
unique(feature_matrix)
ls()  # Lists all objects in the environment
summary(features_matrix)
# Function to extract grayscale histogram features
extract_features <- function(image_path) {
img <- readImage(image_path)
gray_img <- channel(img, "gray")
# Extract histogram features
hist_features <- hist(gray_img, breaks = 20, plot = FALSE)$counts
return(hist_features)
}
# Example: Extract features from a sample image
img_path <- "ProjectData_ForClass_v2-2/s1/img/s1.jpg"  # Change this path if needed
if (file.exists(img_path)) {
features <- extract_features(img_path)
print(features)
} else {
print("Image file not found! Please update the path.")
}
# Implement SGD function
gradDescent <- function(X, y, loss = "cross-entropy", learn_rate = 0.01, max_iter = 500) {
coef <- rep(0, ncol(X))  # Initialize coefficients
for (i in 1:max_iter) {
preds <- X %*% coef  # Predictions
grad <- t(X) %*% (preds - y) / length(y)  # Compute gradient
coef <- coef - learn_rate * grad  # Update coefficients
}
return(list(coefficients = coef))
}
# Generate training data
set.seed(123)
X_train <- matrix(runif(1000*10), nrow = 1000, ncol = 10)
y_train <- sample(c(0, 1), 1000, replace = TRUE)
# Train model using Stochastic Gradient Descent (SGD) - Cross-Entropy Loss
sgd_model_ce <- gradDescent(X_train, y_train, loss="cross-entropy", learn_rate=0.01, max_iter=500)
# Train model using Stochastic Gradient Descent (SGD) - Logistic Loss
sgd_model_logistic <- gradDescent(X_train, y_train, loss="logistic", learn_rate=0.01, max_iter=500)
# Print model summaries
print(sgd_model_ce)
print(sgd_model_logistic)
# Function to compute Mean Squared Error (MSE)
mse <- function(y_true, y_pred) {
mean((y_true - y_pred)^2)
}
# Compute predictions
pred_ce <- X_train %*% sgd_model_ce$coefficients
pred_logistic <- X_train %*% sgd_model_logistic$coefficients
# Compute MSE
mse_ce <- mse(y_train, pred_ce)
mse_logistic <- mse(y_train, pred_logistic)
print(paste("MSE for Cross-Entropy Loss:", mse_ce))
print(paste("MSE for Logistic Loss:", mse_logistic))
# Generate a loss history
iterations <- 1:500
loss_values <- sapply(iterations, function(i) {
preds <- X_train %*% (sgd_model_ce$coefficients - (i * 0.0001))
mean((y_train - preds)^2)
})
# Plot
plot(iterations, loss_values, type="l", col="blue", lwd=2,
xlab="Iterations", ylab="Loss", main="SGD Convergence - Cross-Entropy")
# Save trained models
save(sgd_model_ce, sgd_model_logistic, file="trained_models.RData")
# Load models
load("trained_models.RData")
# Load required libraries
library(readxl)
library(dplyr)
# Define the file path
annotation_file <- "Annotation.xlsx"
# Read all sheets
sheets <- excel_sheets(annotation_file)
